### 📊 Concept of **Covariance** in Statistics

Covariance is a statistical measure that indicates the **direction of the linear relationship** between two variables. It helps you understand how two variables **change together**.

---

### 🔹 Definition:

Covariance measures whether an **increase in one variable** results in an **increase or decrease in another variable**.

* If **X and Y tend to increase together**, covariance is **positive**.
* If **X increases while Y decreases**, covariance is **negative**.
* If there is **no consistent pattern**, covariance is **near zero**.

---

### 🔹 Formula:

For two variables $X$ and $Y$ with $n$ observations:

$$
\text{Cov}(X, Y) = \frac{1}{n} \sum_{i=1}^{n} (X_i - \bar{X})(Y_i - \bar{Y})
$$

Where:

* $X_i, Y_i$ are the data points
* $\bar{X}, \bar{Y}$ are the means of $X$ and $Y$
* $n$ is the number of data points

*Note:* Sometimes $\frac{1}{n-1}$ is used for sample covariance.

---

### 🔹 Interpretation:

| Covariance Value | Interpretation                      |
| ---------------- | ----------------------------------- |
| > 0              | X and Y tend to increase together   |
| < 0              | As X increases, Y tends to decrease |
| ≈ 0              | No linear relationship              |

---

### 🔹 Example:

Let’s say you have data on study hours (X) and test scores (Y):

| Student | Study Hours (X) | Test Score (Y) |
| ------- | --------------- | -------------- |
| A       | 2               | 50             |
| B       | 4               | 60             |
| C       | 6               | 70             |

Means:

* $\bar{X} = 4$
* $\bar{Y} = 60$

Covariance:

$$
\text{Cov}(X,Y) = \frac{1}{3} [(2-4)(50-60) + (4-4)(60-60) + (6-4)(70-60)]
= \frac{1}{3} [(-2)(-10) + 0 + 2(10)] = \frac{1}{3}(20 + 0 + 20) = \frac{40}{3} \approx 13.33
$$

✅ **Positive covariance** ⇒ More study hours → Higher scores.

---

### 🔹 Limitation:

Covariance only shows the **direction** of the relationship, **not the strength** or scale. To interpret strength, we use **correlation**, which is a **normalized** form of covariance.

---

### 🔹 Summary:

| Aspect         | Description                       |
| -------------- | --------------------------------- |
| What it shows  | Direction of linear relationship  |
| Positive value | Variables increase together       |
| Negative value | One increases, other decreases    |
| Near zero      | No linear relationship            |
| Not scaled     | Value depends on units of X and Y |

---

### 📌 Example Data:

We'll use the same example: Study hours (X) and Test scores (Y).

```python
import numpy as np
import matplotlib.pyplot as plt

# Data: Study Hours and Test Scores
X = np.array([2, 4, 6])  # Study hours
Y = np.array([50, 60, 70])  # Test scores

# Calculate the mean of X and Y
mean_X = np.mean(X)
mean_Y = np.mean(Y)

# Manually compute covariance
cov_manual = np.mean((X - mean_X) * (Y - mean_Y))

# Alternatively, use numpy's cov function (gives sample covariance by default)
cov_matrix = np.cov(X, Y, bias=True)  # Set bias=True for population covariance
cov_np = cov_matrix[0, 1]

print(f"Manual Covariance: {cov_manual}")
print(f"Numpy Covariance: {cov_np}")

# Visualization
plt.scatter(X, Y)
plt.title("Scatter Plot: Study Hours vs Test Scores")
plt.xlabel("Study Hours")
plt.ylabel("Test Scores")

# Add trend line
m, b = np.polyfit(X, Y, 1)
plt.plot(X, m*X + b, linestyle='--')

plt.grid(True)
plt.show()
```

---

### 🧮 Output:

You’ll get:

```
Manual Covariance: 13.333333333333334
Numpy Covariance: 13.333333333333334
```

* ✅ The values match: both manual and NumPy calculations are \~13.33.
* ✅ The **scatter plot** shows a clear **upward trend**.
* ✅ The **dashed line** is a trend line (best-fit line) showing positive relationship.

---

Let's now use a **Pandas DataFrame** to calculate **covariance** and then extend it to **correlation**.

---

### ✅ Step-by-step using `pandas`

```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

# Create DataFrame
data = {
    'Study_Hours': [2, 4, 6],
    'Test_Scores': [50, 60, 70]
}
df = pd.DataFrame(data)

# Calculate covariance
cov_value = df['Study_Hours'].cov(df['Test_Scores'])
print(f"Sample Covariance (Pandas): {cov_value:.2f}")

# Calculate correlation
corr_value = df['Study_Hours'].corr(df['Test_Scores'])
print(f"Correlation Coefficient (Pandas): {corr_value:.2f}")

# Scatter plot with trend line
plt.scatter(df['Study_Hours'], df['Test_Scores'], label='Data points')
m, b = np.polyfit(df['Study_Hours'], df['Test_Scores'], 1)
plt.plot(df['Study_Hours'], m*df['Study_Hours'] + b, linestyle='--', label='Trend line')

plt.title("Study Hours vs Test Scores")
plt.xlabel("Study Hours")
plt.ylabel("Test Scores")
plt.grid(True)
plt.legend()
plt.show()
```

---

### 🧾 Output:

```
Sample Covariance (Pandas): 20.00
Correlation Coefficient (Pandas): 1.00
```

> Note: Pandas uses **sample covariance** by default (divides by n−1), so the value is `20` (instead of population covariance \~13.33).

---

### 🔍 Interpretation:

* **Covariance** is positive ⇒ as study hours increase, scores also increase.
* **Correlation = 1.00** ⇒ perfect positive linear relationship (straight line).

---

